
##==============================================================================
##Stuff that only works at Carmen!

# Path to the binary, at Carmen
binary:${TEXTTEST_CHECKOUT}/Testing/TextTest/texttest.py

# Settings for Carmen
checkout_location:~/work
default_checkout:master

# Run with SGE at Carmen
config_module:carmenqueuesystem
queue_system_module:SGE

# ---------------- Values needed to run in batch mode at Carmen -------------------

# Machines where we can start Xvfb
#virtual_display_machine:reedsville
virtual_display_machine:fougamou
virtual_display_machine:singleton

# For each "special batch" run, specify recipients, time limit, architectures and versions 
# Not present means $USER as recipient, no time limit, all versions and all architectures accepted
[batch_recipients]
default:carmen.testtool_tests
local:$USER

[batch_version]
nightly_publish:master
weekly_publish:master
default:i386_linux
default:sparc
default:parisc_2_0
default:lsf
default:sge
default:carmen

[batch_use_collection]
default:true
local:false

# ---------------- Values for the static GUI -------------------

# GUI configuration, drop-down lists to appear in static gui...
[gui_entry_options]
use_checkout:/carm/master
run_this_version:master.i386_linux
run_this_version:master.parisc_2_0
run_this_version:master.sparc
run_this_version:sparc
run_this_version:parisc_2_0
reconnect_to_previous_run:nightjob
request_lsf_queue:waiting
[end]

[batch_result_repository]
default:/carm/proj/texttest/html/TestStateRepository

[historical_report_location]
default:/carm/proj/texttest/html/TestResults
[end]

unsaveable_version:master
unsaveable_version:i386_linux
## ================================================================
## end of stuff only for use at Carmen

[run_dependent_text]
output*:TextTest will write diagnostics
output*:{INTERNAL writedir}{REPLACE <test write dir>}
output*:[^/]*[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9][0-9][0-9]{REPLACE <target tmp dir>}
output:test-case.* on .*new results.*missing results.*differences{WORD -13}
output:test-case.* on .*new results.*differences{WORD -9}
output:test-case.* on .*missing results.*differences{WORD -9}
output:test-case.* on .*new results{WORD -6}
output:test-case.* on .*missing results{WORD -6}
output:test-case.* on .*differences{WORD -5}
output:test-case.* on .*:${WORD -2}
output:second to retry
output:creating batch report{WORD 2}
output:Killing running test{WORD -1}
# Sometimes needed under heavy machine load
output:signal 15
output:signal 9
output:Killed process{WORD 3}
output:Sending mail from{WORD -1}
output:What is your name{LINES 3}
output:^$
targetReport: on .*test-case.*under{WORD -7}
targetReport:^[0-9][0-9][0-9][0-9][0-9][0-9]{WORD 1}
targetReport:{LINE 1}
targetReport:^\>
targetReport:% faster{WORD -2}
# diff output
targetReport:1,2c1
collectReport:^From:{WORD 2}
collectReport:^To:{WORD 2}
collectReport:^Subject:{WORD 2}
catalogue:.nfs
catalogue:[^-]*[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9][0-9][0-9]{REPLACE <texttest tmp dir>}
catalogue:[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9]{REPLACE <today's date>}
# Files that shouldn't mean anything...
catalogue:cmd
catalogue:unixperf
catalogue:.*\.o$
catalogue:i386_linux_opt
# Only filtered on windows
catalogue:tdat.*cmp
catalogue:grep.*cmp
catalogues:{INTERNAL writedir}{REPLACE <test write dir>}
catalogues:[^/]*[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9][0-9][0-9]{REPLACE <target tmp dir>}
errors:RENDER
errors:File .* line .* in
errors:dipjudge.py has not been{WORD -5}
errors:root directory does{WORD -1}
errors:CountTest.__del__
errors:fakeuser under{WORD -1}
errors:TransSocketUNIX
errors:gtk.Combo
errors:Import failed
errors:Warning: Tried to connect to session manager
*gui_log:{INTERNAL writedir}{REPLACE <test write dir>}
*gui_log:[^/]*[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9][0-9][0-9]{REPLACE <target tmp dir>}
# Hard to rely on exactly how much faster things are...
dynamic_gui_log:% faster{WORD 3}
dynamic_gui_log:Running on{WORD -1}
dynamic_gui_log:succeeded on{WORD -1}
dynamic_gui_log:FAILED on{WORD -2}
dynamic_gui_log:time.*:.*sec.
dynamic_gui_log:^\?
dynamic_gui_log:tests started at{WORD -1}
dynamic_gui_log:+ PID{WORD 3}
gui_log:+ PID{WORD 3}
gui_log:Dynamic GUI started at{WORD -1}
gui_log:  File "
# diff output
dynamic_gui_log:1,2c1
jusecaseprops:record{WORD -1}
[end]

[unordered_text]
gui_log:Dynamic run failed{LINES 3}
[end]

[failure_severity]
*gui_log:1
target*:1
shortcut:1
*Report:1
[end]

[diagnostics]
input_directory_variable:TEXTTEST_DIAGNOSTICS
write_directory_variable:TEXTTEST_DIAGDIR
configuration_file:log4py.conf
[end]

# Force recomputation of files on non-UNIX archs
home_operating_system:posix

# Official name of the application
full_name:TextTest

# List of (generally directories) to copy from the test directory to the write directory
# because we want to write there.
partial_copy_test_path:TargetApp
partial_copy_test_path:texttesttmp
partial_copy_test_path:repository
copy_test_path:file_edits

[test_data_environment]
TargetApp:TEXTTEST_HOME
texttesttmp:TEXTTEST_TMP
personaldir:TEXTTEST_PERSONAL_CONFIG
[end]

# List of constant input files to create links to. Will copy them on Windows as symbolic links
# don't exist.
link_test_path:config
link_test_path:FakeBugcli/bugcli
link_test_path:FakeBugcli/bugcli.py
link_test_path:personaldir

# Tell it that we want to track what files are created
create_catalogues:true

# And here's where to find interactive action overrides...
interactive_action_module:texttestgui

# Settings for creation of test cases
use_case_record_mode:GUI

# Want to slow-motion replay the GUI tests
slow_motion_replay_speed:2

# Things to collate
[collate_file]
targetMem:TargetApp/Failures/memory.mem
targetReport:texttesttmp/*/batchreport*
collectReport:texttesttmp/batchreport*
errorsrec:texttesttmp/*/record_errors.log
outputrec:texttesttmp/*/record_run.log
errorsrep:texttesttmp/*/replay_errors.log
outputrep:texttesttmp/*/replay_run.log
jusecaseprops:texttesttmp/*/*/jusecase.properties
filterfile*:TargetApp/filter_files/filter*
overview:report/*/test_[a-z]*.html
overview_all:report/*/test_*_all.html
overview_1day:report/*/test_*_18Jan2006.html
[end]

definition_file_stems:dynamic_usecase
definition_file_stems:record_usecase
definition_file_stems:replay_usecase
definition_file_stems:target_record_usecase
definition_file_stems:target_input

# Run the LSF and SGE tests too...
extra_version:sge
extra_version:lsf
extra_version:carmen

# Automatically collapse successful test suites?
auto_collapse_successful:1

[gui_entry_overrides]
auto-replay_in_dynamic_gui:1
[end]
