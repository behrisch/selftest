# Path to the binary, at Carmen
binary:${TEXTTEST_CHECKOUT}/Testing/TextTest/texttest.py

# Settings for Carmen
checkout_location:~/work
default_checkout:master

# Run with SGE at Carmen
config_module:carmenqueuesystem
queue_system_module:SGE

# ---------------- Values needed to run in batch mode at Carmen -------------------

# Machines where we can start Xvfb
#virtual_display_machine:reedsville
virtual_display_machine:fougamou
virtual_display_machine:singleton

unsaveable_version:master
unsaveable_version:i386_linux
unsaveable_version:lsf
unsaveable_version:sge

# For each "special batch" run, specify recipients, time limit, architectures and versions 
# Not present means $USER as recipient, no time limit, all versions and all architectures accepted
[batch_recipients]
default:carmen.testtool_tests
local:$USER

[batch_version]
nightly_publish:master
weekly_publish:master
default:i386_linux
default:sparc
default:parisc_2_0
default:lsf
default:sge

[batch_use_collection]
default:true
local:false

# ---------------- Values for the static GUI -------------------

# GUI configuration, drop-down lists to appear in static gui...
[gui_entry_options]
use_checkout:/carm/master
run_this_version:master.i386_linux
run_this_version:master.parisc_2_0
run_this_version:master.sparc
run_this_version:sparc
run_this_version:parisc_2_0
reconnect_to_previous_run:nightjob
request_lsf_queue:waiting
[end]

[gui_entry_overrides]
auto-replay_in_dynamic_gui:1
[end]

[run_dependent_text]
output*:checkout{WORD -1}
output:BadApplication{WORD -1}
output:Reconnecting to test results{WORD -1}
output:Faking start
output:not a core dump{WORD 1}
output:test-case.* on .*new results.*missing results.*differences{WORD -13}
output:test-case.* on .*new results.*differences{WORD -9}
output:test-case.* on .*missing results.*differences{WORD -9}
output:test-case.* on .*new results{WORD -6}
output:test-case.* on .*missing results{WORD -6}
output:test-case.* on .*differences{WORD -5}
output:test-case.* on .*:${WORD -2}
output:second to retry
output:creating batch report{WORD 2}
output:Killing running test{WORD -1}
output*:TextTest will write diagnostics
output:matching applications{WORD -1}
# Sometimes needed under heavy machine load
output:signal 15
output:signal 9
output:Killed process{WORD 3}
output:Bug file at{WORD 4}
output:Sending mail from{WORD -1}
output:What is your name{LINES 3}
targetReport: on .*test-case.*under{WORD -7}
targetReport:^[0-9][0-9][0-9][0-9][0-9][0-9]{WORD 1}
targetReport:{LINE 1}
targetReport:^\+
targetReport:^\?
targetReport:% faster{WORD -2}
collectReport:^From:{WORD 2}
collectReport:^To:{WORD 2}
collectReport:^Subject:{WORD 2}
catalogue:.nfs
catalogue:[^-]*[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9][0-9][0-9]{REPLACE <texttest tmp dir>}
catalogue:[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9]{REPLACE <today's date>}
# Files that shouldn't mean anything...
catalogue:cmd
catalogue:unixperf
errors:RENDER
errors:File .* line .* in
errors:dipjudge.py has not been{WORD -5}
errors:root directory does{WORD -1}
errors:CountTest.__del__
errors:fakeuser under{WORD -1}
errors:TransSocketUNIX
errors:gtk.Combo
errors:Import failed
errors:Warning: Tried to connect to session manager
*gui_log:{INTERNAL writedir}{REPLACE <test write dir>}
dynamic_gui_log:[^/]*[0-9][0-9][A-Z][a-z][a-z][0-9][0-9][0-9][0-9][0-9][0-9]{REPLACE <target tmp dir>}
# Hard to rely on exactly how much faster things are...
dynamic_gui_log:% faster{WORD 3}
dynamic_gui_log:Running on{WORD -1}
dynamic_gui_log:succeeded on{WORD -1}
dynamic_gui_log:FAILED on{WORD -2}
dynamic_gui_log:time.*:.*sec.
dynamic_gui_log:^\?
dynamic_gui_log:tests started at{WORD -1}
jusecaseprops:record{WORD -1}
[end]

[unordered_text]
gui_log:Dynamic run failed{LINES 3}
[end]

[failure_severity]
*gui_log:1
target*:1
shortcut:1
[end]

[diagnostics]
input_directory_variable:TEXTTEST_DIAGNOSTICS
write_directory_variable:TEXTTEST_DIAGDIR
configuration_file:log4py.conf
[end]

# Force recomputation of files on non-UNIX archs
home_operating_system:posix

# Official name of the application
full_name:TextTest

# List of (generally directories) to copy from the test directory to the write directory
# because we want to write there.
partial_copy_test_path:TargetApp
partial_copy_test_path:texttesttmp

[test_data_environment]
TargetApp:TEXTTEST_HOME
texttesttmp:TEXTTEST_TMP
[end]

# List of constant input files to create links to. Will copy them on Windows as symbolic links
# don't exist.
link_test_path:config
link_test_path:FakeBugcli/bugcli
link_test_path:FakeBugcli/bugcli.py

# Tell it that we want to track what files are created
create_catalogues:true

# And here's where to find interactive action overrides...
interactive_action_module:texttestgui

# Settings for creation of test cases
use_case_record_mode:console

# Want to slow-motion replay the GUI tests
slow_motion_replay_speed:2

# Things to collate
[collate_file]
targetMem:TargetApp/Failures/memory.mem
targetReport:texttesttmp/*/batchreport*
collectReport:texttesttmp/batchreport*
errorsrec:texttesttmp/*/record_errors.log
outputrec:texttesttmp/*/record_run.log
errorsrep:texttesttmp/*/replay_errors.log
outputrep:texttesttmp/*/replay_run.log
jusecaseprops:texttesttmp/*/*/jusecase.properties
filterfile*:TargetApp/filter_files/filter*
[end]

# This is a hack to be able to test recording, don't compare it if it appears...
discard_file:record_usecase

definition_file_stems:dynamic_usecase
definition_file_stems:record_usecase
definition_file_stems:replay_usecase

# Run the LSF and SGE tests too...
extra_version:sge
extra_version:lsf

[batch_result_repository]
default:/carm/proj/texttest/html/TestStateRepository

[historical_report_location]
default:/carm/proj/texttest/html/TestResults
